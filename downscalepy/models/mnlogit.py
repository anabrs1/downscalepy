"""
Bayesian logit model with PÃ³lya Gamma prior with MCMC

Implementation of multinomial logistic regression using Polya-Gamma latent variables,
following Polson et al. (2013).
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Tuple, Any
from scipy.stats import multivariate_normal
import tqdm
import warnings


def mnlogit(X: np.ndarray, Y: np.ndarray, baseline: Optional[int] = None, 
           niter: int = 1000, nburn: int = 500, A0: float = 1e4,
           calc_marginal_fx: bool = False, max_exp: float = 30.0,
           jitter: float = 1e-6) -> Dict[str, Any]:
    """
    MCMC estimation of a multinomial logit model following Polson et al. (2013).

    Parameters
    ----------
    X : np.ndarray
        An n by k matrix of explanatory variables
    Y : np.ndarray
        An n by p matrix of dependent variables
    baseline : int, optional
        Baseline class for estimation. Parameters will be set to zero.
        Defaults to the p-th column.
    niter : int, default=1000
        Total number of MCMC draws.
    nburn : int, default=500
        Burn-in draws for MCMC. Note: nburn has to be lower than niter.
    A0 : float, default=1e4
        Prior variance scalar for all slope coefficients
    calc_marginal_fx : bool, default=False
        Should marginal effects be calculated?
    max_exp : float, default=30.0
        Maximum value for exponentiation to prevent overflow
    jitter : float, default=1e-6
        Small value added to diagonal of covariance matrix for numerical stability

    Returns
    -------
    Dict[str, Any]
        A dictionary containing:
        - postb: A k x p x (niter - nburn) array containing posterior draws of the slope coefficients.
        - marginal_fx: A k x p x (niter - nburn) array containing posterior draws of marginal effects.
        - X, Y, baseline: The matrices of explanatory and dependent variables, and the baseline class.

    References
    ----------
    Nicholas G. Polson, James G. Scott, and Jesse Windle. Bayesian inference for 
    logistic models using Polya-Gamma latent variables. 
    Journal of the American statistical Association 108.504 (2013): 1339-1349.
    """
    warnings.filterwarnings('ignore', category=RuntimeWarning)
    
    if np.any(np.isnan(X)) or np.any(np.isinf(X)):
        raise ValueError("Input X contains NaN or Inf values")
    if np.any(np.isnan(Y)) or np.any(np.isinf(Y)):
        raise ValueError("Input Y contains NaN or Inf values")
    
    n, k = X.shape
    p = Y.shape[1]
    
    if baseline is None:
        baseline = p - 1
    
    if niter <= nburn:
        raise ValueError("niter has to be higher than nburn.")
    
    X_mean = np.mean(X, axis=0)
    X_std = np.std(X, axis=0)
    X_std[X_std == 0] = 1.0  # Avoid division by zero
    X_normalized = (X - X_mean) / X_std
    
    nn = np.ones((n, p))
    beta_prior_mean = np.zeros((k, p))
    beta_prior_var = np.eye(k) * A0
    
    ndiscard = nburn
    nretain = niter - ndiscard
    
    postb = np.zeros((k, p, nretain))
    
    curr_beta = np.zeros((k, p))
    curr_beta[:, baseline] = 0  # Baseline class parameters set to zero
    curr_xb = X_normalized @ curr_beta
    curr_om = np.zeros((n, p))
    
    beta_prior_var_inv = np.linalg.inv(beta_prior_var + np.eye(k) * jitter)
    kappa = Y - nn / 2
    
    def rpg_approx(n, b, c):
        """
        Approximate Polya-Gamma sampling with numerical safeguards.
        This is a simplified approximation - in production, use a proper PG sampler.
        """
        c_clipped = np.clip(c, -max_exp, max_exp)
        result = np.random.gamma(1, 1, size=n) / (1 + np.exp(-c_clipped))
        result[np.isnan(result) | np.isinf(result)] = 1e-6
        return result
    
    def safe_exp(x):
        """Safe exponentiation to prevent overflow"""
        return np.exp(np.clip(x, -max_exp, max_exp))
    
    pp = list(range(p))
    pp.remove(baseline)  # Remove baseline class
    
    for iter in tqdm.tqdm(range(niter), desc="MCMC Sampling"):
        for j in pp:
            xb_others = X_normalized @ curr_beta[:, np.arange(p) != j]
            xb_others_clipped = np.clip(xb_others, -max_exp, max_exp)
            A = np.sum(safe_exp(xb_others_clipped), axis=1)
            A = np.maximum(A, 1e-10)
            c_j = np.log(A)
            
            eta_j = X_normalized @ curr_beta[:, j] - c_j
            
            curr_om[:, j] = rpg_approx(n, nn[:, j], eta_j)
            
            curr_om[:, j] = np.clip(curr_om[:, j], 1e-10, 1e10)
            
            weighted_X = X_normalized * curr_om[:, j][:, np.newaxis]
            XtX = X_normalized.T @ weighted_X
            V_inv = beta_prior_var_inv + XtX + np.eye(k) * jitter
            
            try:
                V = np.linalg.inv(V_inv)
                V = (V + V.T) / 2  # Make symmetric
                V = V + np.eye(k) * jitter
                
                rhs = beta_prior_var_inv @ beta_prior_mean[:, j]
                rhs = rhs + X_normalized.T @ (kappa[:, j] + c_j * curr_om[:, j])
                b = V @ rhs
                
                if np.any(np.isnan(b)) or np.any(np.isinf(b)) or np.any(np.isnan(V)) or np.any(np.isinf(V)):
                    continue
                
                curr_beta[:, j] = multivariate_normal.rvs(mean=b, cov=V)
                
                if np.any(np.isnan(curr_beta[:, j])) or np.any(np.isinf(curr_beta[:, j])):
                    curr_beta[:, j] = beta_prior_mean[:, j]
            except np.linalg.LinAlgError:
                print(f"Warning: Matrix inversion failed at iteration {iter}, class {j}. Using regularized approach.")
                V = np.linalg.inv(V_inv + np.eye(k) * 0.01)
                b = beta_prior_mean[:, j]  # Fall back to prior mean
                curr_beta[:, j] = b  # Use prior mean instead of sampling
        
        if iter >= ndiscard:
            s = iter - ndiscard
            postb[:, :, s] = curr_beta
            curr_xb = X_normalized @ curr_beta
    
    warnings.resetwarnings()
    
    # Denormalize coefficients
    for i in range(postb.shape[2]):
        for j in range(p):
            if j != baseline:
                postb[:, j, i] = postb[:, j, i] / X_std
    
    marginal_fx = None
    if calc_marginal_fx:
        marginal_fx = np.zeros((k, p, nretain))
        
        for jjj in range(nretain):
            MU = X @ postb[:, :, jjj]
            MU_clipped = np.clip(MU, -max_exp, max_exp)
            exp_MU = safe_exp(MU_clipped)
            sum_exp_MU = np.sum(exp_MU, axis=1)[:, np.newaxis]
            sum_exp_MU = np.maximum(sum_exp_MU, 1e-10)
            pr = exp_MU / sum_exp_MU
            
            for ppp in range(p):
                bbb = np.ones((n, k)) * postb[:, ppp, jjj]
                pr_bbb = bbb.copy()
                
                for kk in range(k):
                    pr_bbb[:, kk] = np.sum(pr * postb[kk, :, jjj], axis=1)
                
                partial1 = pr[:, ppp][:, np.newaxis] * (bbb - pr_bbb)
                if np.any(np.isnan(partial1)) or np.any(np.isinf(partial1)):
                    partial1 = np.zeros_like(partial1)
                marginal_fx[:, ppp, jjj] = np.mean(partial1, axis=0)
    
    results = {
        "postb": postb,
        "marginal_fx": marginal_fx,
        "X": X,
        "Y": Y,
        "baseline": baseline
    }
    
    return results
